{"cells":[{"cell_type":"markdown","metadata":{"id":"UTBNdd6xAE-8"},"source":["### **Installation and Loading**: Packages and Libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W2A-3B6bKWwO"},"outputs":[],"source":["!pip install ndlib"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gm0hLtE2FmzU"},"outputs":[],"source":["import random\n","import json\n","import numpy as np\n","import pandas as pd\n","import networkx as nx\n","import ndlib.models.ModelConfig as mc\n","import ndlib.models.epidemics as ep\n","from ndlib.utils import multi_runs\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as mpatches\n","from pylab import rcParams"]},{"cell_type":"markdown","metadata":{"id":"Vk45xWvlhnYX"},"source":["### **Cloning the Git Repository**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hZjFJ8GZFVXr"},"outputs":[],"source":["!git clone https://github.com/virgiiim/EC_Reddit_CaseStudy\n","!git clone https://github.com/nicollasro/Echochamber"]},{"cell_type":"markdown","metadata":{"id":"m-GdtcT8X1vI"},"source":["### **Structuring the Echo Chambers:** Reddit"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m6TDlWGgNYCq"},"outputs":[],"source":["def load_echochambers(topic):\n","  networks = []\n","  for snapshot in ['2017-01-01_2017-07-01','2017-07-01_2018-01-01','2018-01-01_2018-07-01','2018-07-01_2019-01-01','2019-01-01_2019-07-01']:\n","    df_edges = pd.read_csv(\"/content/EC_Reddit_CaseStudy/data/\"+topic+'/'+topic+'_'+snapshot+'_edgelist.csv',header=0)\n","    G = nx.from_pandas_edgelist(df_edges,'from_id', 'to_id', ['weight'])\n","    dict_nodes_label = pd.read_csv(\"/content/EC_Reddit_CaseStudy/data/\"+topic+'/'+topic+'_'+snapshot+'_nodelist.csv', index_col='id')['leaning'].to_dict()\n","    nx.set_node_attributes(G, dict_nodes_label, 'label')\n","    connected_components = list(nx.connected_components(G))\n","    largest_component = max(connected_components, key=len)\n","    G = G.subgraph(largest_component)\n","    networks.append(G)\n","  return networks"]},{"cell_type":"markdown","metadata":{"id":"Esoc5dJ_F1MY"},"source":["### **Selection of Initial Nodes:** Random, Closeness, Degree, Betweenness, Information, Eigenvector"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S2hWl3Rd2OM5"},"outputs":[],"source":["def find_initial_nodes(graph, nodes_quantity):\n","\n","    random_nodes=random.sample(list(graph.nodes()), nodes_quantity)\n","    closeness_central_nodes = sorted(nx.closeness_centrality(graph).items(), key=lambda x: x[1], reverse=True)[:nodes_quantity]\n","    closeness_central_nodes = [node[0] for node in closeness_central_nodes]\n","    betweenness_central_nodes = sorted(nx.betweenness_centrality(graph,weight=\"weight\").items(), key=lambda x: x[1], reverse=True)[:nodes_quantity]\n","    betweenness_central_nodes = [node[0] for node in betweenness_central_nodes]\n","    degree_central_nodes = sorted(nx.degree_centrality(graph).items(), key=lambda x: x[1], reverse=True)[:nodes_quantity]\n","    degree_central_nodes = [node[0] for node in degree_central_nodes]\n","    information_central_nodes = sorted(nx.information_centrality(graph,weight=\"weight\").items(), key=lambda x: x[1], reverse=True)[:nodes_quantity]\n","    information_central_nodes = [node[0] for node in information_central_nodes]\n","    return [random_nodes, closeness_central_nodes, betweenness_central_nodes, degree_central_nodes, information_central_nodes]"]},{"cell_type":"markdown","metadata":{"id":"mt7HmuogLVgr"},"source":["### **Models:** Independent Cascades Propagation, Profile and Profile-Threshold"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"szLiV8FfInqH"},"outputs":[],"source":["def configura_probabilidades(G, model_name, initial_nodes, profile, blocked_nodes):\n","    low_propagation_prob = 0.1\n","    high_propagation_prob =0.9\n","    config = mc.Configuration()\n","    if model_name ==\"IndependentCascadesModel\":\n","      for edge in G.edges():\n","          config.add_edge_configuration(\"probability\", edge, high_propagation_prob)\n","      for node in initial_nodes:\n","          neighbors = list(G.neighbors(node))\n","          for neighbor in neighbors:\n","              if neighbor in initial_nodes:\n","                  config.add_edge_configuration(\"probability\", (node, neighbor), low_propagation_prob)\n","      return config\n","\n","    elif model_name ==\"ProfileThresholdModel\":\n","      print(model_name)\n","      config.add_model_initial_configuration(\"Blocked\", blocked_nodes)\n","      print(\"Blocked Nodes: \", blocked_nodes)\n","      config.add_model_initial_configuration(\"Infected\", initial_nodes)\n","      print(\"Initial Nodes: \",initial_nodes)\n","      threshold = 0\n","      for i in G.nodes():\n","          config.add_node_configuration(\"threshold\", i, threshold)\n","          config.add_node_configuration(\"profile\", i, profile)\n","      print(config.get_model_configuration())\n","      return config\n","\n","    else:\n","      print(model_name)\n","      config.add_model_initial_configuration(\"Blocked\", blocked_nodes)\n","      config.add_model_initial_configuration(\"Infected\", initial_nodes)\n","      for i in G.nodes():\n","        config.add_node_configuration(\"profile\", i, profile)\n","      print(\"Configuration: \", config.get_model_configuration())\n","      return config\n","\n","def run_simulation(G, model_name, initial_nodes, config, execution_rounds, iterations, total_nodes):\n","    if model_name ==\"IndependentCascadesModel\":\n","      model = ep.IndependentCascadesModel(G)\n","      model.set_initial_status(config)\n","      trends = multi_runs(model, execution_number=execution_rounds, iteration_number=iterations, infection_sets=initial_nodes, nprocesses=4)\n","      return trends\n","\n","    else:\n","      influence_evolution = []\n","      for i in range(execution_rounds):\n","        model = ep.ProfileModel(G)\n","        model.set_initial_status(config)\n","        iterations_set = model.iteration_bunch(iterations,)\n","        influence_list = collect_node_counts(iterations_set, total_nodes)\n","        influence_evolution.append(influence_list)\n","      return influence_evolution\n","\n","def collect_node_counts(iterations_set, total_nodes):\n","    node_counts = []\n","    for entry in iterations_set:\n","        if 'node_count' in entry:\n","          node_counts.append(entry['node_count'][1]/total_nodes)\n","    return node_counts\n","\n","def find_stability_position(lists):\n","    positions = []\n","    for sublist in lists:\n","        position = len(sublist)\n","        for i in range(len(sublist) - 1):\n","            if all(x == sublist[i] for x in sublist[i:]):\n","                position = i\n","                break\n","        positions.append(position)\n","    return positions"]},{"cell_type":"markdown","metadata":{"id":"PwBwK5xGdaaT"},"source":["### **Main Program:** No Blocking or Random Blocking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lj_M9Z1-ePGC"},"outputs":[],"source":["topics = ['politics','guncontrol','minority']\n","topic_index = 2\n","networks = load_echochambers(topic = topics[topic_index])\n","\n","model_name= [\"IndependentCascadesModel\",'ProfileThresholdModel', 'Profile']\n","model_num = 2\n","inicial_nodes_logic = [\"aleatory\", \"closeness\",\"betweeness\", \"degree\", \"information\"]\n","inicial_nodes_type = 4\n","print(\"Criterion: \", inicial_nodes_logic[inicial_nodes_type])\n","\n","iterations = 100\n","execution_rounds = 30\n","inicial_percentage = 0.025\n","block_percentage = 0.05\n","print(\"Model: \", model_name[model_num])\n","print(\"Topic: \", topics[topic_index])\n","\n","influence_percentage_list=[]\n","instability_iteration = []\n","for profile in [0.9, 0.5]:\n","  for snapshot in range(5):\n","    n_nodes = networks[snapshot].nodes\n","    total_nodes = len(n_nodes)\n","    nodes_quantity = int(inicial_percentage * total_nodes)\n","    initial_nodes_list = find_initial_nodes(networks[snapshot], nodes_quantity)\n","    initial_nodes = initial_nodes_list[inicial_nodes_type]\n","\n","    available_nodes = list(set(n_nodes) - set(initial_nodes_list[inicial_nodes_type]))\n","    blocked_nodes_quantity = int(total_nodes*block_percentage)\n","    blocked_nodes = random.sample(available_nodes, blocked_nodes_quantity)\n","    print(\"Inicial Nodes: \", initial_nodes)\n","    print('Blocked Nodes Quantity:', len(blocked_nodes))\n","    config = configura_probabilidades(networks[snapshot], model_name[model_num], initial_nodes, profile, blocked_nodes)\n","    influence_evolution = run_simulation(networks[snapshot], model_name[model_num], initial_nodes, config, execution_rounds, iterations, total_nodes)\n","    print(influence_evolution)\n","    stability_positions = find_stability_position(influence_evolution)\n","    print(stability_positions)\n","    instability_iteration.append(stability_positions)\n","    influence_percentage_list.append(influence_evolution)\n","    print(influence_percentage_list)"]},{"cell_type":"markdown","metadata":{"id":"AlI0LjqORnF2"},"source":["### **Main Program:** Selective Blocking"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fly7cuY5RY2Y"},"outputs":[],"source":["topics = ['politics','guncontrol','minority']\n","topic_index = 2\n","networks = load_echochambers(topic = topics[topic_index])\n","\n","model_name= [\"IndependentCascadesModel\",'ProfileThresholdModel', 'Profile']\n","model_num = 2\n","inicial_nodes_logic = [\"aleatory\", \"closeness\",\"betweeness\", \"degree\", \"information\"]\n","inicial_nodes_type = 4\n","print(\"Criterion: \", inicial_nodes_logic[inicial_nodes_type])\n","\n","iterations = 100\n","execution_rounds = 30\n","inicial_percentage = 0.025\n","block_percentage = 0.05\n","print(\"Model: \", model_name[model_num])\n","print(\"Topic: \", topics[topic_index])\n","\n","with open(\"diff_strategic_nodes_\"+topics[topic_index]+\".json\", 'r') as f:\n","  blocked_nodes = json.load(f)\n","\n","influence_percentage_list=[]\n","instability_iteration = []\n","for profile in [0.9, 0.5]:\n","  for snapshot in range(5):\n","    n_nodes = networks[snapshot].nodes\n","    total_nodes = len(n_nodes)\n","    nodes_quantity = int(inicial_percentage * total_nodes)\n","    initial_nodes_list = find_initial_nodes(networks[snapshot], nodes_quantity)\n","    initial_nodes = initial_nodes_list[inicial_nodes_type]\n","\n","    config = configura_probabilidades(networks[snapshot], model_name[model_num], initial_nodes, profile, blocked_nodes[snapshot])\n","    influence_evolution = run_simulation(networks[snapshot], model_name[model_num], initial_nodes, config, execution_rounds, iterations, total_nodes)\n","    print(influence_evolution)\n","    stability_positions = find_stability_position(influence_evolution)\n","    print(stability_positions)\n","    instability_iteration.append(stability_positions)\n","    influence_percentage_list.append(influence_evolution)\n","    print(influence_percentage_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yTuyHI9bCj9I"},"outputs":[],"source":["inicial_nodes_logic =[\"aleatory\", \"closeness\",\"betweeness\", \"degree\", \"information\"]\n","nodes_labels = [\"$S_{1} (Pfl=0.9)$\", \"$S_{2} (Pfl=0.9)$\", \"$S_{3} (Pfl=0.9)$\", \"$S_{4} (Pfl=0.9)$\", \"$S_{5} (Pfl=0.9)$\",\"$S_{1} (Pfl=0.5)$\", \"$S_{2} (Pfl=0.5)$\", \"$S_{3} (Pfl=0.5)$\", \"$S_{4} (Pfl=0.5)$\", \"$S_{5} (Pfl=0.5)$\"]\n","colors = ['green', 'green', 'green', 'green', 'green','orange', 'orange', 'orange', 'orange', 'orange']\n","markers= [\"o\", \"^\", \"s\", \"<\", \"*\", \"o\", \"^\", \"s\", \"<\", \"*\"]\n","linestyles = ['-', '--', ':', '-.', (0, (3, 1, 1, 1, 1, 1)),'-', '--', ':', '-.', (0, (3, 1, 1, 1, 1, 1))]\n","\n","\n","def plot_lines(influence_percentage_list):\n","    plt.figure(figsize=(7, 6))\n","    for i in range(10):\n","      mean = [sum(element) / len(influence_percentage_list[i]) for element in zip(*influence_percentage_list[i])]\n","      num_iterations = [i for i in range(iterations)]\n","      plt.plot(num_iterations, mean, label=nodes_labels[i],color=colors[i],\n","               linestyle=linestyles[i],\n","               )\n","\n","    plt.xlabel('Iterations',fontsize=20)\n","    plt.ylabel('% of Network Affected',fontsize=20)\n","    plt.xticks(np.arange(0, iterations+1, 15))\n","    plt.grid(True)\n","    plt.tick_params(labelsize=20)\n","    plt.subplots_adjust(left=0.125, bottom=0.18, right=0.9, top=0.888, wspace=0.2, hspace=0.2)\n","\n","    ax = plt.gca()\n","    for axis in ['top','bottom','left','right']:\n","      ax.spines[axis].set_linewidth(2)\n","      ax.spines[axis].set_color('black')\n","    plt.subplots_adjust(left=0.2)\n","    plt.savefig('/content/Evolutional_blocking_strategic_'+model_name[model_num]+'_'+topics[topic_index]+'_'+inicial_nodes_logic[inicial_nodes_type]+'.pdf',bbox_inches='tight')\n","    plt.show()\n","\n","plot_lines(influence_percentage_list)"]},{"cell_type":"markdown","metadata":{"id":"8jWCZg_YDwhJ"},"source":["### **Nodes Triviality Analysis**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CCH5Ktghg0yc"},"outputs":[],"source":["def get_sorted_nodes_by_centrality(G, centrality_func):\n","    centrality = centrality_func(G)\n","    return sorted(centrality, key=centrality.get, reverse=True)\n","\n","def calculate_probability(nodes_to_check, sorted_nodes, percentile):\n","    top_percentile_count = int(len(sorted_nodes) * percentile)\n","    if top_percentile_count < len(nodes_to_check):\n","        return 0.0\n","    top_percentile_nodes = sorted_nodes[:top_percentile_count]\n","    present_count = sum(1 for node in nodes_to_check if node in top_percentile_nodes)\n","    return present_count / len(nodes_to_check)\n","\n","def calculate_confidence_interval(probability, n, confidence=0.95):\n","    interval = z * np.sqrt((probability * (1 - probability)) / n)\n","    lower_bound = np.clip(probability - interval, 0, 1)\n","    upper_bound = np.clip(probability + interval, 0, 1)\n","    return (lower_bound, upper_bound)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yn4tKm2SU0Qh"},"outputs":[],"source":["topics = ['politics','guncontrol','minority']\n","topic_index = 2\n","with open(\"strategic_nodes_\"+topics[topic_index]+\".json\", 'r') as f:\n","  blocked_nodes = json.load(f)\n","networks = load_echochambers(topic = topics[topic_index])\n","\n","for snapshot in range(5):\n","  G = networks[snapshot]\n","\n","  nodes_to_check = blocked_nodes[snapshot]\n","\n","  centrality_functions = {\n","      'Degree': nx.degree_centrality,\n","      'Closeness': nx.closeness_centrality,\n","      'Betweenness': nx.betweenness_centrality,\n","      'PageRank': lambda G: nx.pagerank(G, alpha=0.85),\n","      'farmonic': nx.harmonic_centrality,\n","  }\n","\n","  percentiles = [0.05, 0.10, 0.15, 0.20]\n","\n","  results = []\n","\n","  for centrality_name, centrality_func in centrality_functions.items():\n","      sorted_nodes = get_sorted_nodes_by_centrality(G, centrality_func)\n","      for p in percentiles:\n","          probability = calculate_probability(nodes_to_check, sorted_nodes, p)\n","          conf_interval = calculate_confidence_interval(probability, len(nodes_to_check))\n","          margin_of_error = conf_interval[1] - probability\n","          results.append((centrality_name, p, probability*100, margin_of_error*100))\n","\n","  centralities = list(centrality_functions.keys())\n","  num_centralities = len(centralities)\n","  x_labels = [f\"{int(p * 100)}%\" for p in percentiles]\n","\n","  probabilities = np.array([prob for _, _, prob, _ in results]).reshape(num_centralities, len(percentiles))\n","  errors = np.array([err for _, _, _, err in results]).reshape(num_centralities, len(percentiles))\n","\n","  colors = [\"lightgray\",\"gray\",\"green\",\"goldenrod\",\"darkkhaki\"]\n","\n","  fig, ax = plt.subplots(figsize=(7, 6))\n","  bar_width = 0.15\n","  index = np.arange(len(percentiles))\n","  for i, (centrality, color) in enumerate(zip(centralities, colors)):\n","      ax.bar(index + i * bar_width, probabilities[i], bar_width, yerr=errors[i], label=centrality, capsize=5, color=color)\n","  ax.set_xticks(index + bar_width * (num_centralities - 1) / 2)\n","  ax.set_xticklabels(x_labels)\n","  tamanho = 25\n","  plt.tick_params(labelsize=tamanho)\n","  plt.xlabel('Percentil', fontsize=tamanho)\n","  plt.ylabel('Probability (%)',fontsize=tamanho)\n","  ax.set_ylim(0, 105)\n","  plt.subplots_adjust(left=0.125, bottom=0.18, right=0.9, top=0.888, wspace=0.2, hspace=0.2)\n","  ax = plt.gca()\n","  for axis in ['top','bottom','left','right']:\n","    ax.spines[axis].set_linewidth(2)\n","    ax.spines[axis].set_color('black')\n","  plt.subplots_adjust(left=0.2)\n","  plt.savefig('/content/Percentil_'+topics[topic_index]+'_'+str(snapshot)+'.pdf',bbox_inches='tight')\n","\n","  plt.show()\n"]}],"metadata":{"colab":{"collapsed_sections":["UTBNdd6xAE-8","Vk45xWvlhnYX","yLeo4ZpJ37kz"],"provenance":[{"file_id":"1A9K_-uq8KwpjTOj0cNCVAPymfu0ZRPgB","timestamp":1760537526303},{"file_id":"1gGAyKLpwBVQgHskz8YnjEJWSpg2ze1-M","timestamp":1716567595499},{"file_id":"1aFW6-qVh-wRoUelMYuIwlrRFBuo8T1ya","timestamp":1715975298782},{"file_id":"13EJiFGFj12uuOU1jmJPLlbQUFl7C8xrs","timestamp":1715786198094},{"file_id":"1NE17Og7cHDA5UZSojkT4vUYwO4W91Kap","timestamp":1714850132427},{"file_id":"1DbyI57EP6tTJCSQEUcgjrn2gw9-oyaxw","timestamp":1713212202717},{"file_id":"1snDkoWuGQi2QIwdTgLQSQZGNlx9ycGmt","timestamp":1711057607317},{"file_id":"1DGq65gtEcZdjFxa3fwhBhcdS_oQdgNKS","timestamp":1710505689794},{"file_id":"1nSYvkIp1nQuUuts_Cj6fTEYsVg-hYy3u","timestamp":1707328037486},{"file_id":"1vdiSYBb7Du3Nd0OiCXQNPL4V8dMtpEow","timestamp":1706046450709},{"file_id":"1jPeHgPKGbLbgWEB2t9bm1k-SYxVCl3fn","timestamp":1699354253584}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}